{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "V3 Tensorflow mlchallengefirst.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PZ0FhWVM550m"
      },
      "source": [
        "# Importacion de datos\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZSdPIXO1Cobs"
      },
      "source": [
        "# data manipulation\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow_datasets as tfds\n",
        "from google.colab import files\n",
        "\n",
        "# plot\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# machine learning frameworks\n",
        "import tensorflow as tf\n",
        "import torch\n",
        "from torch.autograd import Variable\n",
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "import tensorflow_hub as hub\n",
        "\n",
        "\n",
        "# python\n",
        "import datetime\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.utils import to_categorical\n",
        "\n",
        "plt.style.use('tableau-colorblind10')\n",
        "# plt.style.use('dark_background')\n",
        "from sklearn.metrics import confusion_matrix, precision_score, accuracy_score, recall_score, f1_score, roc_auc_score\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rqSEfg-AfFR1"
      },
      "source": [
        "importa datos"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wdauS9DzC4nc"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HqcHx8SNDG9z"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T98UgtOYDPXq"
      },
      "source": [
        "social = pd.read_csv('/content/social.csv')\n",
        "matriculaciones = pd.read_csv('/content/matriculaciones.csv')\n",
        "academicos = pd.read_csv('/content/academicos.csv')\n",
        "quiz = pd.read_csv('/content/quiz.csv')\n",
        "entregas = pd.read_csv('/content/entregas.csv')\n",
        "lecciones = pd.read_csv('/content/lecciones.csv')\n",
        "foros = pd.read_csv('/content/foros.csv')\n",
        "abandonos = pd.read_csv('/content/abandonos.csv')\n",
        "abandonos_test = pd.read_csv('/content/abandonos_test.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FHmFX1U4LEqH"
      },
      "source": [
        "variable = 'last' #con esta variable podemos cambiar si queremos los datos del ultimo año o del primero\n",
        "#de todas maneras la mayorias de las variables que usamos son promedios de todos los años "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9P5LBjjifLD4"
      },
      "source": [
        "obtiene datos "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hvH5omZ86EKU"
      },
      "source": [
        "# Tratamiento de datos"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wCHdWtLAL3uV"
      },
      "source": [
        "foros['año'] = foros['fecha'].apply(lambda x: pd.to_datetime(x).year)\n",
        "participacion_en_foros = foros.groupby(['id','año'])['fecha'].count().reset_index(name='cantidad_foro')\n",
        "cantidad_foro = participacion_en_foros.groupby(['id'])['cantidad_foro'].mean().reset_index(name='cantidad_foro')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u9vmfcadwA2r"
      },
      "source": [
        "#entregas.calificacion[entregas['calficacion'] < 0] = 0\n",
        "entregas.loc[(entregas['calficacion'] < 0) | (entregas['calficacion'].isnull()), 'calficacion'] = 0\n",
        "quiz.loc[(quiz['grado'] < 0) | (quiz['grado'].isnull()), 'grado'] = 0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UcWnD5mcRupx"
      },
      "source": [
        "entregas['año'] = entregas['fecha'].apply(lambda x: pd.to_datetime(x).year)\n",
        "entregas_total = entregas.groupby(['id','año'])['fecha'].count().reset_index(name='cantidad')\n",
        "entregas_total = entregas_total.groupby(['id'])['cantidad'].mean().reset_index(name='cantidad_entregas')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tbEMtx3YvKGB"
      },
      "source": [
        "entregas_calificacion = entregas.groupby(['id'])['calficacion'].mean().reset_index(name='calificacion')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OW8YIllwvSHw"
      },
      "source": [
        "entregas_calificacion['calificacion'].fillna(value=0)\n",
        "entregas_calificacion['calificacion'].astype(int)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gmv5M7OaRZlQ"
      },
      "source": [
        "quiz['año'] = quiz['fecha_inicio'].apply(lambda x: pd.to_datetime(x).year)\n",
        "participacion_quiz = quiz.groupby(['id','año'])['fecha_inicio'].count().reset_index(name='cantidad_quiz')\n",
        "participacion_quiz = participacion_quiz.groupby(['id'])['cantidad_quiz'].mean().reset_index(name='quiz_entregas')\n",
        "notas_quiz = quiz.groupby(['id'])['grado'].mean().reset_index(name='quiz_calificacion')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aiONlpcatkg7"
      },
      "source": [
        "notas_quiz['quiz_calificacion'].describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DLRoo9juGtW1"
      },
      "source": [
        "# primero convertimos las fechas de string a un int que representa solo el año (prescindimos del mes/dia/hora/etc)\n",
        "social['FS_fec'] = social['FS_fec'].apply(lambda x: pd.to_datetime(x).year)\n",
        "social['SD_fn'] = social['SD_fn'].apply(lambda x: pd.to_datetime(x).year)\n",
        "# decimos que la edad es la diferencia del año en que se registra el dato, y el año de nacimiento\n",
        "social['edad'] = social['FS_fec'] - social['SD_fn']\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JNGDpZwcT7Ad"
      },
      "source": [
        "## Cantidad de hijos"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e25Te502T_h0"
      },
      "source": [
        "social['SD_hijoQ'].fillna(0)\n",
        "social['SD_hijoQ'] = social['SD_hijoQ'] / social['SD_hijoQ'].max()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H_bqqc7RUvxn"
      },
      "source": [
        "social['SD_hijoQ'].describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3pODrZgEVENL"
      },
      "source": [
        "Fecha de nacimiento de los Hijos"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KEH4C03YVEwT"
      },
      "source": [
        "fig = plt.figure(figsize=(15, 5))\n",
        "\n",
        "#hijos = ['HJ_fn_1', 'HJ_fn_2', 'HJ_fn_3']\n",
        "#i = 0\n",
        "#for h in hijos:\n",
        " # social[social[hijos[i]] > 0][hijos[i]].plot.hist(title='Distribución de Edades de Hijos', bins=10)\n",
        " # i=i+1\n",
        "\n",
        "social[social['HJ_fn_1'] > 0]['HJ_fn_1'].plot.hist(title='Distribución de Edades de Hijos', bins=10)\n",
        "social[social['HJ_fn_2'] > 0]['HJ_fn_2'].plot.hist(title='Distribución de Edades de Hijos', bins=10)\n",
        "social[social['HJ_fn_3'] > 0]['HJ_fn_3'].plot.hist(title='Distribución de Edades de Hijos', bins=10)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pGaX3MLiVN3s"
      },
      "source": [
        "Hay un estudiante que tiene un hijo nacido en 1920, eliminamos el registro"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jB1h2DLGVY9r"
      },
      "source": [
        "social = pd.concat([social[social['HJ_fn_1'] > 1930], social[social['HJ_fn_1'].isna()]])\n",
        "social = pd.concat([social[social['HJ_fn_2'] > 1930], social[social['HJ_fn_2'].isna()]])\n",
        "social = pd.concat([social[social['HJ_fn_3'] > 1930], social[social['HJ_fn_3'].isna()]])\n",
        "fig = plt.figure(figsize=(15, 5))\n",
        "hijos = ['HJ_fn_1', 'HJ_fn_2', 'HJ_fn_3']\n",
        "i = 0\n",
        "for h in hijos:\n",
        "  social[social[hijos[i]] > 0][hijos[i]].plot.hist(title='Distribución de Edades de Hijos', bins=10)\n",
        "  i=i+1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fPnCk0NCVgjM"
      },
      "source": [
        "Los normalizamos entre 0 y 1 tomando como año máximo 2019 y mínimo 1930"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J7nC8jc-Vh_e"
      },
      "source": [
        "social['HJ_fn_1'] = social['HJ_fn_1'].apply(lambda x: x/2019)\n",
        "social['HJ_fn_2'] = social['HJ_fn_2'].apply(lambda x: x/2019)\n",
        "social['HJ_fn_3'] = social['HJ_fn_3'].apply(lambda x: x/2019)\n",
        "\n",
        "fig = plt.figure(figsize=(15, 5))\n",
        "\n",
        "social[social['HJ_fn_1'] > 0]['HJ_fn_1'].plot.hist(title='Distribución de Edades de Hijos', bins=10)\n",
        "social[social['HJ_fn_2'] > 0]['HJ_fn_2'].plot.hist(title='Distribución de Edades de Hijos', bins=10)\n",
        "social[social['HJ_fn_3'] > 0]['HJ_fn_3'].plot.hist(title='Distribución de Edades de Hijos', bins=10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YluNakEfdjeS"
      },
      "source": [
        "## Fecha de nacimiento de los hijos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Op_6PhZBUun6"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0SehycCiHWk5"
      },
      "source": [
        "academicos.loc[(academicos['inasistencia'] < 0) | (academicos['inasistencia'].isnull()), 'inasistencia'] = 0\n",
        "academicos['año'] = academicos['periodo_id'].apply(lambda x: int(x.split('-')[0]))\n",
        "academicos\n",
        "calificaciones = academicos[academicos['calificacion'] >= 0]\n",
        "calificaciones = calificaciones.groupby(['id'])['calificacion'].mean().reset_index(name='promedio')\n",
        "inasistencias = academicos.groupby(['id'])['inasistencia'].sum().reset_index(name='inasistencia')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gTlL1E0-3gLs"
      },
      "source": [
        "calificaciones"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uaNLjrW0OQiS"
      },
      "source": [
        "n = 0\n",
        "mismo_departamento = pd.DataFrame(columns=('id', 'mismo_dep'))\n",
        "for alumno in social['id'].unique().tolist():\n",
        "   itr = social[social['id'] == alumno]   \n",
        "   mismo_dep = ((itr['SD_ln_de'] == 'Durazno').any() & (itr['DP_itr'] == 'ITR Centro Sur').any()) or ((itr['SD_ln_de'] == 'Río Negro').any() & (itr['DP_itr'] == 'ITR Suroeste').any()) or ((itr['SD_ln_de'] == 'Rivera').any() & (itr['DP_itr'] == 'ITR Norte').any()) or ((itr['DP_ca'].str.contains('Licenciatura en Ciencia y Tecnología de Lácteos')).any() & (itr['SD_ln_de'] == 'Colonia').any()) or ((itr['DP_ca'].str.contains('Licenciatura en Análisis Alimentario')).any() & (itr['SD_ln_de'] == 'Paysandú').any()) or ((itr['DP_ca'].str.contains('Tecnólogo en Manejo de Sistemas')).any() & (itr['SD_ln_de'] == 'Colonia').any()) or ((itr['DP_ca'].str.contains('Licenciatura en Leche')).any() & (itr['SD_ln_de'] == 'Colonia').any()) or ((itr['DP_ca'].str.contains('Tecnólogo en Jazz')).any() & (itr['SD_ln_de'] == 'Florida').any())\n",
        "   if mismo_dep:\n",
        "    mismo_departamento.loc[n] = [alumno,1]\n",
        "    n = n+1\n",
        "   else:\n",
        "    mismo_departamento.loc[n] = [alumno,0]\n",
        "    n = n+1\n",
        "#aqui creamos una variable binaria q seria si vive o no en el mismo departamento donde estudias\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nmOVpfpuhcCb"
      },
      "source": [
        "n = 0 \n",
        "materias_aprobacion = pd.DataFrame(columns=('id', 'materias_desaprobadas', 'materias_aprobadas'))\n",
        "for alumno in academicos['id'].unique().tolist():\n",
        "   calificacion = academicos[academicos['id']==alumno]   \n",
        "   calificacion.dropna()\n",
        "   calificacion_desaprobadas = calificacion[calificacion['calificacion']<3]\n",
        "   calificacion_aprobadas = calificacion[calificacion['calificacion']>3]\n",
        "   materias_aprobacion.loc[n] = [alumno, calificacion_desaprobadas['calificacion'].count(), calificacion_aprobadas['calificacion'].count()]\n",
        "   n = n+1\n",
        "#aqui estariamos contando cuantas materias aprobo o reprobo cada alumno "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xzxOU8qsgQAD"
      },
      "source": [
        "n = 0\n",
        "weight = pd.DataFrame(columns=('id', 'weight'))\n",
        "for alumno in abandonos['id'].unique().tolist():\n",
        "   abandono_alumno = abandonos[abandonos['id'] == alumno]   \n",
        "   if (abandono_alumno['abandono'] == 1).any():\n",
        "    weight.loc[n] = [alumno,1.0]\n",
        "    n = n+1\n",
        "   else:\n",
        "    weight.loc[n] = [alumno,0.2]\n",
        "    n = n+1\n",
        "#creamos esta columna para definir el \"peso\" de cada registro, como tenemos menos alumnos que dejaron, les damos una mayor relevancia "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SHZu61-fojkX"
      },
      "source": [
        "datos = matriculaciones"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KnvPOe9YqPPZ"
      },
      "source": [
        "datos = pd.merge(datos, social, on='id', how='left')\n",
        "#merge es parecido a los join de las bases de datos, on indica la columna sobre la cual se los une\n",
        "#el how seria como el tipo de join "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tkbw5vP-mtO6"
      },
      "source": [
        "datos = pd.merge(datos, entregas_total, on='id', how='left')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wMS2_BLUEzHC"
      },
      "source": [
        "datos = pd.merge(datos, participacion_en_foros, on='id', how='left')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8yWbP1MtItqI"
      },
      "source": [
        "datos = pd.merge(datos, calificaciones, on='id', how='left')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qvGLdlTISPiQ"
      },
      "source": [
        "datos = pd.merge(datos, participacion_quiz, on='id', how='left')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TveErYjeqRNY"
      },
      "source": [
        "datos = pd.merge(datos, mismo_departamento, on='id', how='left')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BdKhGYTbWCmt"
      },
      "source": [
        "datos = pd.merge(datos, entregas_calificacion, on='id', how='left')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nn09Qf7QzqJE"
      },
      "source": [
        "datos = pd.merge(datos, notas_quiz, on='id', how='left')\n",
        "datos = pd.merge(datos, inasistencias, on='id', how='left')\n",
        "datos = pd.merge(datos, weight, on='id', how='left')\n",
        "datos = pd.merge(datos, materias_aprobacion, on='id', how='left')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "debrH_2AWs7g"
      },
      "source": [
        "datos"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BymOd1Dj32s6"
      },
      "source": [
        "datos.edad = datos.edad.fillna(value=0)#fillna completa los valores nulos para evitar errores\n",
        "datos.edad = datos.edad.astype(int)\n",
        "datos.promedio = datos.promedio.fillna(value=0)\n",
        "datos.promedio = datos.promedio.astype(int)\n",
        "datos.DP_ca = datos.DP_ca.fillna(value='Desconocido')\n",
        "datos.DP_ca = datos.DP_ca.astype(str)\n",
        "datos.SD_sex = datos.SD_sex.fillna(value='Desconocido')\n",
        "datos.SD_re_de = datos.SD_re_de.fillna(value='Desconocido')\n",
        "datos.SD_re_de = datos.SD_re_de.astype(str)\n",
        "datos.SD_sex = datos.SD_sex.astype(str)\n",
        "datos.DP_itr = datos.DP_itr.fillna(value='Desconocido')\n",
        "datos.DP_itr = datos.DP_itr.astype(str)\n",
        "datos.ED_ms_o = datos.ED_ms_o.fillna(value='Desconocido')\n",
        "datos.ED_ms_o = datos.ED_ms_o.astype(str)\n",
        "datos.ED_rep = datos.ED_rep.fillna(value='Desconocido')\n",
        "datos.ED_rep = datos.ED_rep.astype(str)\n",
        "datos.SD_fi_ne = datos.SD_fi_ne.fillna(value='Desconocido')\n",
        "datos.SD_fi_ne = datos.SD_fi_ne.astype(str)\n",
        "datos.cantidad_entregas = datos.cantidad_entregas.fillna(value='0')\n",
        "datos.cantidad_entregas = datos.cantidad_entregas.astype(int)\n",
        "datos.quiz_entregas = datos.quiz_entregas.fillna(value=0)\n",
        "datos.quiz_entregas = datos.quiz_entregas.astype(int)\n",
        "datos.SD_fi_tt = datos.SD_fi_tt.fillna(value=0)\n",
        "datos.SD_fi_tt = datos.SD_fi_tt.astype(int)\n",
        "datos.SD_fi_th = datos.SD_fi_th.fillna(value=0)\n",
        "datos.SD_fi_th = datos.SD_fi_th.astype(int)\n",
        "datos.EP_m = datos.EP_m.fillna(value='Desconocido')\n",
        "datos.EP_m = datos.EP_m.astype(str)\n",
        "datos.EP_p = datos.EP_p.fillna(value='Desconocido')\n",
        "datos.EP_p = datos.EP_p.astype(str)\n",
        "datos.TR_ct = datos.TR_ct.fillna(value='Desconocido')\n",
        "datos.TR_ct = datos.TR_ct.astype(str)\n",
        "datos.cantidad_foro = datos.cantidad_foro.fillna(value=0)\n",
        "datos.cantidad_foro = datos.cantidad_foro.astype(int)\n",
        "datos.VH_to = datos.VH_to.fillna(value='Desconocido')\n",
        "datos.VH_to = datos.VH_to.astype(str)\n",
        "datos.SD_hijos = datos.SD_hijos.fillna(value=0)\n",
        "datos.SD_hijos = datos.SD_hijos.astype(int)\n",
        "datos.ED_auaa = datos.ED_auaa.fillna(value=0)\n",
        "datos.ED_auaa = datos.ED_auaa.astype(int)\n",
        "datos.mismo_dep = datos.mismo_dep.fillna(value=0)\n",
        "datos.mismo_dep = datos.mismo_dep.astype(int)\n",
        "datos.calificacion = datos.calificacion.fillna(value=0)\n",
        "datos.calificacion = datos.calificacion.astype(int)\n",
        "datos.quiz_calificacion = datos.quiz_calificacion.fillna(value=0)\n",
        "datos.quiz_calificacion = datos.quiz_calificacion.astype(int)\n",
        "datos.inasistencia = datos.inasistencia.fillna(value=0)\n",
        "datos.inasistencia = datos.inasistencia.astype(int)\n",
        "datos.plan_id = datos.plan_id.fillna(value='Desconocido')\n",
        "datos.plan_id = datos.plan_id.astype(str)\n",
        "datos.TR_rtc = datos.TR_rtc.fillna(value=0)\n",
        "datos.TR_rtc = datos.TR_rtc.astype(int)\n",
        "datos.materias_desaprobadas = datos.materias_desaprobadas.fillna(value=0)\n",
        "datos.materias_desaprobadas = datos.materias_desaprobadas.astype(int)\n",
        "datos.materias_aprobadas = datos.materias_aprobadas.fillna(value=0)\n",
        "datos.materias_aprobadas = datos.materias_aprobadas.astype(int)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HQq9aFxlgfKl"
      },
      "source": [
        "datos.SD_hijoQ = datos.SD_hijoQ.fillna(value=0)\n",
        "datos.SD_hijoQ = datos.SD_hijoQ.astype(int)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sItoNPPmhJtg"
      },
      "source": [
        "datos.SD_hijoQ"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5V-fRByMobWx"
      },
      "source": [
        "datos['plan_id'].unique()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qkio7tJHKWqP"
      },
      "source": [
        "datos_test = pd.merge(abandonos_test, datos, on='id', how='left')\n",
        "datos = pd.merge(abandonos, datos, on='id', how='left')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bm2p-LA_h0Z5"
      },
      "source": [
        "merge convina los datos, parecido a un join de base de datos de ahi el left/rigth/inner etc "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tyxMCYe0f6Rg"
      },
      "source": [
        "elimnamos datos nulos y unificamos el tipo de datos para evitar errores"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JqcF5aVmoQX8"
      },
      "source": [
        "datos_test = datos_test.drop_duplicates(keep=variable, subset='id')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QlwjApZxoDRq"
      },
      "source": [
        "datos = datos.drop_duplicates(keep=variable, subset='id')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3xLvwju1mrTw"
      },
      "source": [
        "entregas_calificacion"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UBZXhrRUQkAI"
      },
      "source": [
        "social['SD_ln_de'].unique()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fhrGmUFpRI-x"
      },
      "source": [
        "social['DP_itr'].unique()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4iZZTpkjh6k4"
      },
      "source": [
        "elimino los datos de otros años y me quedo con los del ultimo o primer año para simplificar, cambiando last por first o viceversa "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "In97LHLjust1"
      },
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "scaler = MinMaxScaler(feature_range=(0,1))\n",
        "scaler.fit_transform(datos.promedio.values.reshape(-1, 1))\n",
        "scaler.fit_transform(datos.inasistencia.values.reshape(-1, 1))\n",
        "scaler.fit_transform(datos.materias_desaprobadas.values.reshape(-1, 1))\n",
        "scaler.fit_transform(datos.materias_aprobadas.values.reshape(-1, 1))\n",
        "scaler.fit_transform(datos.quiz_calificacion.values.reshape(-1, 1))\n",
        "scaler.fit_transform(datos.quiz_entregas.values.reshape(-1, 1))\n",
        "scaler.fit_transform(datos.edad.values.reshape(-1, 1))\n",
        "scaler.fit_transform(datos.cantidad_foro.values.reshape(-1, 1))\n",
        "scaler.fit_transform(datos.cantidad_entregas.values.reshape(-1, 1))\n",
        "scaler.fit_transform(datos.TR_rtc.values.reshape(-1, 1))\n",
        "scaler.fit_transform(datos.SD_hijoQ.values.reshape(-1, 1))\n",
        "scaler.fit_transform(datos.ED_auaa.values.reshape(-1, 1))\n",
        "scaler.fit_transform(datos.SD_fi_th.values.reshape(-1, 1))\n",
        "scaler.fit_transform(datos.SD_fi_tt.values.reshape(-1, 1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WEx-j0LaLVT_"
      },
      "source": [
        "\n",
        "\n",
        "columnas_datos = ['weight','materias_aprobadas', 'materias_desaprobadas','inasistencia','TR_rtc','plan_id','calificacion','quiz_calificacion','mismo_dep','SD_hijos','SD_hijoQ','ED_auaa','SD_fi_ne','id','SD_re_de', 'SD_sex', 'VH_to','edad','promedio','DP_itr','ED_ms_o','ED_rep','DP_ca', 'quiz_entregas','SD_fi_tt', 'SD_fi_th','EP_m','EP_p','TR_ct', 'cantidad_foro', 'cantidad_entregas']\n",
        "x = datos[columnas_datos]\n",
        "y = datos['abandono']\n",
        "x_prueba = datos_test[columnas_datos]\n",
        "\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VNFTXr-JgA8y"
      },
      "source": [
        "crea las datos x e y"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p3DLQ3wbb8zI"
      },
      "source": [
        "datos_abandono = datos[datos['abandono']==1]\n",
        "datos_no_abandono = datos[datos['abandono'] ==0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yVaB6NUygEiz"
      },
      "source": [
        "creo dos dataset, uno con todos los que abandonaron y otro con los que no para comparar datos en busqueda de buenas variables\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7IV17euHdbdQ"
      },
      "source": [
        "#datos_abandono['SD_fi_ne'].describe()\n",
        "datos_abandono['TR_rtc'].value_counts().plot(kind='bar')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W9-toj5dezet"
      },
      "source": [
        "datos['TR_rtc'].unique()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d56czveBdun1"
      },
      "source": [
        "#datos_no_abandono['SD_fi_ne'].describe()\n",
        "datos_no_abandono['TR_rtc'].value_counts().plot(kind='bar')\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3a61GzAEnISN"
      },
      "source": [
        "datos_abandono"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JtR80bIzw6v0"
      },
      "source": [
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ltYk05NEt0Yf"
      },
      "source": [
        "x_train"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RAbt_kDAXBsO"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YS5dyiVA6M5d"
      },
      "source": [
        "# Creacion de columnas tf"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O6QBxsWyPJnA"
      },
      "source": [
        "#el modelo dnn usa este tipo de columnas\n",
        "\n",
        "sex = tf.feature_column.categorical_column_with_vocabulary_list('SD_sex',['f, m, Desconocido'])#este tipo de columna es para categorias con pocas opciones\n",
        "SD_sex = tf.feature_column.indicator_column(sex)#la convertimos en columna indicafor para que lo entienda tensorflow\n",
        "re_de = tf.feature_column.categorical_column_with_hash_bucket('SD_re_de', hash_bucket_size=1000)#este tipo de columna es para categorias con muchas opciones\n",
        "SD_re_de = tf.feature_column.embedding_column(re_de, dimension=19)#tambien hay que transformarlo a este tipo para q lo entienda tensorflow estimator\n",
        "edad = tf.feature_column.numeric_column('edad')#tipo de columna numerica\n",
        "promedio = tf.feature_column.numeric_column('promedio')\n",
        "DP_c = tf.feature_column.categorical_column_with_hash_bucket('DP_ca', hash_bucket_size=1000)\n",
        "DP_ca = tf.feature_column.embedding_column(DP_c, dimension=24)\n",
        "DP = tf.feature_column.categorical_column_with_vocabulary_list('DP_itr',['ITR Norte', 'ITR Suroeste', 'Desconocido', 'ITR Centro Sur'])\n",
        "DP_itr = tf.feature_column.indicator_column(DP)\n",
        "ED_ms = tf.feature_column.categorical_column_with_vocabulary_list('ED_ms_o', ['centifica', 'humanistica', 'bachillerato-tecnologico',\n",
        "       'Desconocido', 'biologica', 'Seleccione una opción',\n",
        "       'cursos-tecnicos-nivel-medio', 'arte-expresion'])\n",
        "ED_ms_o = tf.feature_column.indicator_column(ED_ms)\n",
        "rep = tf.feature_column.categorical_column_with_hash_bucket('ED_rep', hash_bucket_size=1000)\n",
        "ED_rep = tf.feature_column.embedding_column(rep, dimension=5)\n",
        "cantidad_entregas = tf.feature_column.numeric_column('cantidad_entregas')\n",
        "quiz_entregas = tf.feature_column.numeric_column('quiz_entregas')\n",
        "SD_fi_tt = tf.feature_column.numeric_column('SD_fi_tt')\n",
        "SD_fi_th = tf.feature_column.numeric_column('SD_fi_th')\n",
        "EPp = tf.feature_column.categorical_column_with_hash_bucket('EP_p', hash_bucket_size=1000)\n",
        "EP_p = tf.feature_column.embedding_column(EPp, dimension=11)\n",
        "EPm = tf.feature_column.categorical_column_with_hash_bucket('EP_m', hash_bucket_size=1000)\n",
        "EP_m = tf.feature_column.embedding_column(EPm, dimension=11)\n",
        "TR = tf.feature_column.categorical_column_with_hash_bucket('TR_ct', hash_bucket_size=1000)\n",
        "TR_ct = tf.feature_column.embedding_column(TR, dimension=10)\n",
        "cantidad_foro = tf.feature_column.numeric_column('cantidad_foro')\n",
        "VH = tf.feature_column.categorical_column_with_hash_bucket('VH_to', hash_bucket_size=1000)\n",
        "VH_to = tf.feature_column.embedding_column(VH, dimension=8)\n",
        "SD_hijos = tf.feature_column.numeric_column('SD_hijos')\n",
        "SD_hijoQ = tf.feature_column.numeric_column('SD_hijoQ')\n",
        "ED_auaa = tf.feature_column.numeric_column('ED_auaa')\n",
        "SD_fi = tf.feature_column.categorical_column_with_vocabulary_list('SD_fi_ne', ['posgrado_completo_o_incompleto', 'terciarios_completos',\n",
        "       'secundaria_completa_o_incompleta', 'Desconocido',\n",
        "       'terciarios_incompletos', 'primaria_o_menos'])\n",
        "SD_fi_ne = tf.feature_column.indicator_column(SD_fi, )\n",
        "mismo_dep = tf.feature_column.numeric_column('mismo_dep')\n",
        "quiz_calificacion = tf.feature_column.numeric_column('quiz_calificacion')\n",
        "entregas_calificacion = tf.feature_column.numeric_column('calificacion')\n",
        "inasistencias = tf.feature_column.numeric_column('inasistencia')\n",
        "plan = tf.feature_column.categorical_column_with_hash_bucket('plan_id', hash_bucket_size=1000)\n",
        "plan_id = tf.feature_column.embedding_column(plan, dimension=30)\n",
        "\n",
        "TR_rtc = tf.feature_column.numeric_column('TR_rtc')\n",
        "weight = tf.feature_column.numeric_column('weight')\n",
        "materias_aprobadas = tf.feature_column.numeric_column('materias_aprobadas')\n",
        "materias_desaprobadas = tf.feature_column.numeric_column('materias_desaprobadas')\n",
        " "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t8VDI3jKba2K"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QunMac0qH4B4"
      },
      "source": [
        "f =tf.estimator.BaselineClassifier(n_classes=3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5d9uiAhhH3BE"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7VP17tiphiqD"
      },
      "source": [
        "crea los tensores de tensorflow q seria como sus columnas, sequence column seria para los valores string tipo departamento, y numeric para valores numericos, categorical para cuando son pocos valores y ya los sabes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QHjJF-Rc9jzn"
      },
      "source": [
        "# Columnas a usar"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U_o5BmFwYtq7"
      },
      "source": [
        "#aqui van las columnas que efectivamente se van a usar en el algoritmo \n",
        "columnas = [weight,materias_aprobadas,materias_desaprobadas,ED_ms_o,inasistencias,TR_rtc,plan_id,mismo_dep,quiz_entregas,promedio,entregas_calificacion, edad,SD_fi_ne, SD_hijos, SD_hijoQ, DP_ca, EP_m,SD_sex,SD_re_de, ED_rep, DP_itr,cantidad_entregas ]\n",
        "\n",
        "#aqui son las mismas columnas pero quito la columna weight\n",
        "columnas_para_clasificacion = [inasistencias,materias_aprobadas,materias_desaprobadas,TR_rtc,plan_id,mismo_dep,quiz_entregas,promedio,entregas_calificacion, edad,SD_fi_ne, SD_hijos, SD_hijoQ, DP_ca, EP_m,SD_sex,SD_re_de, ED_rep, DP_itr,cantidad_entregas ]\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JHy4KTKlCdJS"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sYgp-veYgQLU"
      },
      "source": [
        "lista las columnas"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "teo2D-pJY4oB"
      },
      "source": [
        "funcion_entrada = tf.estimator.inputs.pandas_input_fn(x=x_train, y=y_train, batch_size=100, num_epochs=None, shuffle=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tl-oJmCH_Pw5"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mIc4D5mjgTNy"
      },
      "source": [
        "rellena los tensores"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dJI3D6U36UMc"
      },
      "source": [
        "# Creacion de algoritmo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e_ZgWWuqaCIy"
      },
      "source": [
        "'''modelo = tf.estimator.LinearClassifier(feature_columns=columnas,optimizer='Adagrad')\n",
        "  #algoritmo linear\n",
        " \n",
        "modelo = tf.estimator.DNNClassifier(\n",
        " feature_columns=columnas_sin_weight, #modelo deep neural network\n",
        " hidden_units=[1024,254,36],#estos numeros son la cantidad de 'neuronas' por capa, por ejemplo 3 capas de 60/10/2 etc                                     \n",
        " optimizer=tf.train.ProximalAdagradOptimizer(learning_rate=0.01,\n",
        "                  #          l1_regularization_strength=0.001,\n",
        "                              #                          l2_regularization_strength=0.001\n",
        "\n",
        "),\n",
        " n_classes=2,#numero de clasificaciones a predecir\n",
        " dropout=0.1,\n",
        " model_dir='./tmp/mnist_model',\n",
        " #optimizer=tf.train.AdamOptimizer(1e-4,),#optimizador del algoritmo y algoritmo, hay varios a elegir en la documentacion\n",
        " weight_column=weight,\n",
        " )\n",
        "'''\n",
        "\n",
        "\n",
        "#este ultimo seria como una convinacion de ambos algoritmos linear y dnn, de todas maneras no obtuvimos mejoras grandes\n",
        "modelo = tf.estimator.DNNLinearCombinedClassifier(\n",
        "    linear_feature_columns=columnas_para_clasificacion,\n",
        "    linear_optimizer=tf.train.ProximalAdagradOptimizer(learning_rate=0.01),\n",
        "    dnn_feature_columns=columnas_para_clasificacion,\n",
        "    dnn_hidden_units=[1024,252,42],\n",
        "    dnn_dropout=0.1,\n",
        "    dnn_optimizer=tf.train.ProximalAdagradOptimizer(learning_rate=0.01),\n",
        "    weight_column = weight\n",
        "   )\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o-moYaX1gWIo"
      },
      "source": [
        "selecciona el algoritmo a usar"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G2QXes-V_YQw"
      },
      "source": [
        "# Entrenamiento del algoritmo "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WCjcRu-faYO_"
      },
      "source": [
        "modelo.train(input_fn=funcion_entrada, steps=8000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6lAzVmqB8JJ5"
      },
      "source": [
        "modelo.evaluate"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vmnFL7pOgbI7"
      },
      "source": [
        "entrena el algoritmo, mientras menor sea el 'loss' mejor esta haciendo el sistema. Steps es la cantidad de veces que se entrena, mas steps mejora el sistema"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yefJbLJR3He5"
      },
      "source": [
        "funcion_prediccion = tf.estimator.inputs.pandas_input_fn(x=x_test,batch_size=len(x_test), shuffle=False)\n",
        "funcion_prediccion_prueba = tf.estimator.inputs.pandas_input_fn(x=x_prueba,batch_size=len(x_prueba), shuffle=False)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w8CfHwrXggKh"
      },
      "source": [
        "hace la prediccion"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e1We1s_73v28"
      },
      "source": [
        "generador_predicciones = modelo.predict(input_fn=funcion_prediccion)\n",
        "#aqui estamos prediciendo sobre los datos de entrenamiento\n",
        "\n",
        "generador_predicciones_prueba = modelo.predict(input_fn=funcion_prediccion_prueba)\n",
        "#aqui sobre los datos para entregar"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gWeG6unAAi3z"
      },
      "source": [
        "generador_predicciones_prueba"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MdV6u2F1gkhb"
      },
      "source": [
        "pasa la prediccion a una lista para luego separar las predicciones etc \n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W3OxqWBM38e1"
      },
      "source": [
        "predicciones = list(generador_predicciones)\n",
        "predicciones_prueba = list(generador_predicciones_prueba)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Wj6tYC64Nvk"
      },
      "source": [
        "predicciones_finales = [prediccion['class_ids'][0] for prediccion in predicciones]\n",
        "\n",
        "#la prediccion devuelve objetos en donde el atributo class_ids seria el valor predicho"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jJuGRXLY_pvV"
      },
      "source": [
        "predicciones_finales_prueba = [prediccion['class_ids'][0] for prediccion in predicciones_prueba]\n",
        "\n",
        "#las clases terminadas en _prueba serian las de el abandonos test que son para entregar"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u5pHGnbmgpr7"
      },
      "source": [
        "toma el valor 'y' de casa prediccion"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ggQ218G5A3E"
      },
      "source": [
        "from sklearn.metrics import classification_report"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yrIa08MN6aYk"
      },
      "source": [
        "# Resultados"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mxVj8qPYiUcu"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H6Jt26iF5H5r"
      },
      "source": [
        "print(classification_report(y_test,predicciones_finales))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uu4wrtz1kHfW"
      },
      "source": [
        "diferentes metricas sobre la presicion"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xcSxZNGz_sx1"
      },
      "source": [
        "from sklearn.metrics import confusion_matrix, precision_score, accuracy_score, recall_score, f1_score, roc_auc_score\n",
        "matriz = confusion_matrix(y_test, predicciones_finales)\n",
        "print(matriz)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KIUM2mC-gupK"
      },
      "source": [
        "imprime resultados valores 1 acertados sobre uno errados y luego \n",
        "\n",
        "---\n",
        "\n",
        "valores 0 errados sobre 0 acertados\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WC0AhIZyjc0y"
      },
      "source": [
        "roc_auc = roc_auc_score(y_test, predicciones_finales)\n",
        "print(roc_auc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bm-yF2jchN_B"
      },
      "source": [
        "es un valor del 0 al 1, mientras mas cerca del 1 mas exacto es el modelo de prediccines"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tfQ2Nhqv7TIk"
      },
      "source": [
        "datos_prediccion = pd.DataFrame(columns=['id', 'abandono'] )\n",
        "datos_prediccion_prueba = pd.DataFrame(columns=['id', 'abandono'] )\n",
        "\n",
        "#aqui se genera datasets que completaremos con datos para hacer el cvs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fSb-O7AeQRnh"
      },
      "source": [
        "x=0\n",
        "for i in x_test['id']:\n",
        "  datos_prediccion.loc[x] = [i,predicciones_finales[x]]\n",
        "  x = x+1 \n",
        "  \n",
        "#completamos el dataframe  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sD3u20Y9Pits"
      },
      "source": [
        "x=0\n",
        "for i in x_prueba['id']:\n",
        "  datos_prediccion_prueba.loc[x] = [i,predicciones_finales_prueba[x]]\n",
        "  x = x+1\n",
        "  \n",
        "#completamos el dataframe con los datos para el cvs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1NI2WVhd_-zr"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VYN1wj8j_L6L"
      },
      "source": [
        "archivo_resultados = datos_prediccion_prueba.to_csv('resultados.csv')\n",
        "#generamos el cvs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V_xI4V2JCT2k"
      },
      "source": [
        "#files.download('resultados.csv')\n",
        "#descargamos el cvs, ejecutar por separado"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}